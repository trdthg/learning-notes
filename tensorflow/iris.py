import time
# -*- coding:UTF-8 -*-
import tensorflow as tf
from sklearn import datasets
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split
first_time = time.time()

print('---begin---')
print('å¯¼å…¥æ•°æ®é›†')
x_data = datasets.load_iris().data
y_data = datasets.load_iris().target

# print('æ•°æ®æ®ä¹±åº')
# np.random.seed(116)
# np.random.shuffle(x_data)
# np.random.seed(116)
# np.random.shuffle(y_data)
# tf.random.set_seed(116)
# print('åˆ†å‰²ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†')
# x_train = x_data[:-30]
# y_train = y_data[:-30]
# x_test = x_data[-30:]
# y_test = y_data[-30:]

x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.5, random_state=432)

# ä¿è¯è¾“å…¥çš„ç‰¹å¾å’Œæ ‡ç­¾ç±»å‹ç›¸åŒ, é˜²æ­¢çŸ©é˜µç›¸ä¹˜æŠ¥é”™
x_train = tf.cast(x_train, tf.float32)
x_test = tf.cast(x_test, tf.float32)

# è¾“å…¥ç‰¹å¾ä¸æ ‡ç­¾ä¸€ä¸€å¯¹åº”
train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(1)
test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(1)

# ç”Ÿæˆç¥ç»ç½‘ç»œçš„å‚æ•°
w1 = tf.Variable(tf.random.truncated_normal([4,3], stddev=0.1, seed=1))
b1 = tf.Variable(tf.random.truncated_normal([3], stddev=0.1, seed=1))

# æŒ‡æ•°è¡°å‡å­¦ä¹ ç‡
train_loss_results = []
test_acc = []  

lr_base = 0.1
lr_decay = 0.99
lr_step = 1
epoch = 100
loss_all = 0

# print('---å¼€å§‹è®­ç»ƒ(æ‘¸é±¼)---')
for epoch in range(epoch):
    lr = lr_base * lr_decay ** (epoch / lr_step)
    for step, (x_train, y_train) in enumerate(train_db):
        with tf.GradientTape() as tape:
            y = tf.matmul(x_train, w1) + b1               # ç¥ç»ç½‘ç»œä¹˜åŠ è¿ç®—
            y = tf.nn.softmax(y)                          # ä½¿è¾“å‡ºyç¬¦åˆæ¦‚ç‡åˆ†å¸ƒï¼ˆæ­¤æ“ä½œåä¸ç‹¬çƒ­ç åŒé‡çº§ï¼Œå¯ç›¸å‡æ±‚lossï¼‰
            y_ = tf.one_hot(y_train, depth=3)             # å°†æ ‡ç­¾å€¼è½¬æ¢ä¸ºç‹¬çƒ­ç æ ¼å¼ï¼Œæ–¹ä¾¿è®¡ç®—losså’Œaccuracy
            # print(step,y)
            loss = tf.reduce_mean(tf.square(y_ - y))      # é‡‡ç”¨å‡æ–¹è¯¯å·®æŸå¤±å‡½æ•°mse = mean(sum(y-out)^2)
            # print(y-y_, loss)
            loss_all += loss.numpy()                      # å°†æ¯ä¸ªstepè®¡ç®—å‡ºçš„lossç´¯åŠ ï¼Œä¸ºåç»­æ±‚losså¹³å‡å€¼æä¾›æ•°æ®ï¼Œè¿™æ ·è®¡ç®—çš„lossæ›´å‡†ç¡®
        #è®¡ç®—losså¯¹å„ä¸ªå‚æ•°çš„æ¢¯åº¦
        grads = tape.gradient(loss, [w1, b1])
        # å®ç°æ¢¯åº¦æ›´æ–° w1 = w1 - lr * w1_grad    b = b - lr * b_grad
        w1.assign_sub(lr * grads[0])
        b1.assign_sub(lr * grads[1])
    # print('---ä¸€ä¸ªepochå·²ç» over---')

    # print(f'{epoch}, {loss_all/4}   Î£(ã£Â°Ğ” Â°;)ã£ğŸŸ')

    train_loss_results.append(loss_all/4)
    # print('---å¼€å§‹æµ‹è¯•---')
    total_correct, total_number = 0, 0
    for x_test, y_test in test_db:
        # ç”¨æ›´æ–°åçš„å‚æ•°è¿›è¡Œé¢„æµ‹
        # print(y_test)
        y = tf.matmul(x_test, w1) + b1
        # print(y)
        y = tf.nn.softmax(y)
        # print(y)
        pred = tf.argmax(y, axis=1)  # è¿”å›yä¸­æœ€å¤§å€¼çš„ç´¢å¼•, åŠé¢„æµ‹çš„åˆ†ç±»
        # print(pred)
        # å°†predè½¬æ¢ä¸ºy_testçš„æ•°æ®ç±»å‹
        pred = tf.cast(pred, dtype=y_test.dtype)
        # è‹¥åˆ†ç±»æ­£ç¡®,correct=1, å¦åˆ™ä¸º0, å°†boolè½¬æ¢ä¸ºint
        # print(pred, y_test)
        correct = tf.cast(tf.equal(pred, y_test), dtype=tf.int32)
        # print(correct)
        correct = tf.reduce_sum(correct)
        total_correct += int(correct)
        total_number += x_test.shape[0]
    acc = total_correct/total_number
    test_acc.append(acc)
    print(f'test_acc: {acc}  {lr} {loss_all}')
    loss_all = 0
second_time = time.time()
print(first_time-second_time)

plt.figure()
plt.title('Loss Function Curve')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.plot(train_loss_results, label='$Loss$')
plt.legend()

plt.figure()
plt.title('Acc Curve')
plt.xlabel('Epoch')
plt.ylabel('Acc')
plt.plot(test_acc, label='$Accuracy$')
plt.legend()
plt.show()