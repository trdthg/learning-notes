import time
# -*- coding:UTF-8 -*-
import tensorflow as tf
from sklearn import datasets
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split
first_time = time.time()

print('---begin---')
print('å¯¼å…¥æ•°æ®é›†')
x_data = datasets.load_iris().data
y_data = datasets.load_iris().target
print(x_data)
print(y_data)

print('æ•°æ®æ®ä¹±åº')
np.random.seed(116)
np.random.shuffle(x_data)
np.random.seed(116)
np.random.shuffle(y_data)
tf.random.set_seed(116)
print('åˆ†å‰²ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†')
x_train = x_data[:-30]
y_train = y_data[:-30]
x_test = x_data[-30:]
y_test = y_data[-30:]

# x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=116)

# ä¿è¯è¾“å…¥çš„ç‰¹å¾å’Œæ ‡ç­¾ç±»å‹ç›¸åŒ, é˜²æ­¢çŸ©é˜µç›¸ä¹˜æŠ¥é”™
x_train = tf.cast(x_train, tf.float32)
x_test = tf.cast(x_test, tf.float32)

# è¾“å…¥ç‰¹å¾ä¸æ ‡ç­¾ä¸€ä¸€å¯¹åº”
train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)
test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)

# ç”Ÿæˆç¥ç»ç½‘ç»œçš„å‚æ•°
w1 = tf.Variable(tf.random.truncated_normal([4,3], stddev=0.1, seed=1))
b1 = tf.Variable(tf.random.truncated_normal([3], stddev=0.1, seed=1))

# æŒ‡æ•°è¡°å‡å­¦ä¹ ç‡
train_loss_results = []
test_acc = []  


# -----ä¼˜åŒ–å™¨-----
# lrå­¦ä¹ ç‡è‡ªæ›´æ–°
lr_base, lr_decay, lr_step = 0.1, 0.99, 1
# ä¼˜åŒ–å™¨æ¢¯åº¦æ›´æ–°
# 1.SGDM
# m_w, m_b, beta = 0, 0, 0.9
# 2.ADAGRAD
# v_w, v_b = 0, 0
# 3.RMSProp
# v_w, v_b, beta = 0, 0, 0.9
# 4.adam
m_w, m_b, beta1 = 0, 0, 0.9
v_w, v_b, beta2 = 0, 0, 0.999
delta_w, delta_b = 0, 0
global_step = 0  # æ€»batchæ•°


epoch = 500
loss_all = 0

# print('---å¼€å§‹è®­ç»ƒ(æ‘¸é±¼)---')
for epoch in range(epoch):
    # lr = lr_base * lr_decay ** (epoch / lr_step)
    lr = 0.1
    for step, (x_train, y_train) in enumerate(train_db):
        global_step += 1
        with tf.GradientTape() as tape:
            y = tf.matmul(x_train, w1) + b1               # ç¥ç»ç½‘ç»œä¹˜åŠ è¿ç®—
            y = tf.nn.softmax(y)                          # ä½¿è¾“å‡ºyç¬¦åˆæ¦‚ç‡åˆ†å¸ƒï¼ˆæ­¤æ“ä½œåä¸ç‹¬çƒ­ç åŒé‡çº§ï¼Œå¯ç›¸å‡æ±‚lossï¼‰
            y_ = tf.one_hot(y_train, depth=3)             # å°†æ ‡ç­¾å€¼è½¬æ¢ä¸ºç‹¬çƒ­ç æ ¼å¼ï¼Œæ–¹ä¾¿è®¡ç®—losså’Œaccuracy
            loss = tf.reduce_mean(tf.square(y_ - y))      # é‡‡ç”¨å‡æ–¹è¯¯å·®æŸå¤±å‡½æ•°mse = mean(sum(y-out)^2)
            loss_all += loss.numpy()                      # å°†æ¯ä¸ªstepè®¡ç®—å‡ºçš„lossç´¯åŠ ï¼Œä¸ºåç»­æ±‚losså¹³å‡å€¼æä¾›æ•°æ®ï¼Œè¿™æ ·è®¡ç®—çš„lossæ›´å‡†ç¡®
        grads = tape.gradient(loss, [w1, b1])

        # å®ç°æ¢¯åº¦æ›´æ–°
        # sgd é»˜è®¤
        # w1.assign_sub(lr * grads[0])
        # b1.assign_sub(lr * grads[1])

        # 1. sgdmä¼˜åŒ–å™¨  æ”¹è¿›ä¸€é˜¶åŠ¨é‡
        # m_w = beta * m_w + (1-beta) * grads[0]
        # m_b = beta * m_b + (1-beta) * grads[1]
        # w1.assign_sub(lr * grads[0])
        # b1.assign_sub(lr * grads[1])

        # 2. adagradä¼˜åŒ–å™¨  å¢åŠ äºŒé˜¶åŠ¨é‡
        # v_w += tf.square(grads[0])
        # v_b += tf.square(grads[1])
        # w1.assign_sub(lr * grads[0] / tf.sqrt(v_w))
        # b1.assign_sub(lr * grads[1] / tf.sqrt(v_b))

        #3. RMSPropä¼˜åŒ–å™¨ å¢åŠ äºŒé˜¶åŠ¨é‡
        # v_w += beta * v_w + (1-beta) * tf.square(grads[0])
        # v_b += beta * v_b + (1-beta) * tf.square(grads[1])
        # w1.assign_sub(lr * grads[0] / tf.sqrt(v_w))
        # b1.assign_sub(lr * grads[1] / tf.sqrt(v_b))

        #4. adamä¼˜åŒ–å™¨ NB ç”¨å°±å¯¹äº†
        m_w = beta1 * m_w + (1-beta1) * grads[0]
        m_b = beta1 * m_b + (1-beta1) * grads[1]
        v_w = beta2 * v_w + (1-beta2) * tf.square(grads[0])
        v_b = beta2 * v_b + (1-beta2) * tf.square(grads[1])
        
        m_w_correction = m_w / (1-tf.pow(beta1, int(global_step)))
        m_b_correction = m_b / (1-tf.pow(beta1, int(global_step)))
        v_w_correction = v_w / (1-tf.pow(beta2, int(global_step)))
        v_b_correction = v_b / (1-tf.pow(beta2, int(global_step)))
        w1.assign_sub(lr * m_w_correction / tf.sqrt(v_w_correction))
        b1.assign_sub(lr * m_b_correction / tf.sqrt(v_b_correction))
        
    # print('---ä¸€ä¸ªepochå·²ç» over---')
    # print(f'{epoch}, {loss_all/4}   Î£(ã£Â°Ğ” Â°;)ã£ğŸŸ')

    train_loss_results.append(loss_all/4)
    print(f'test_acc:   {lr} {loss_all}')
    loss_all = 0
    # print('---å¼€å§‹æµ‹è¯•---')
    total_correct, total_number = 0, 0
    for x_test, y_test in test_db:
        # ç”¨æ›´æ–°åçš„å‚æ•°è¿›è¡Œé¢„æµ‹
        y = tf.matmul(x_test, w1) + b1
        y = tf.nn.softmax(y)
        pred = tf.argmax(y, axis=1)  # è¿”å›yä¸­æœ€å¤§å€¼çš„ç´¢å¼•, åŠé¢„æµ‹çš„åˆ†ç±»
        # å°†predè½¬æ¢ä¸ºy_testçš„æ•°æ®ç±»å‹
        pred = tf.cast(pred, dtype=y_test.dtype)
        # è‹¥åˆ†ç±»æ­£ç¡®,correct=1, å¦åˆ™ä¸º0, å°†boolè½¬æ¢ä¸ºint
        correct = tf.cast(tf.equal(pred, y_test), dtype=tf.int32)
        correct = tf.reduce_sum(correct)
        total_correct += int(correct)
        total_number += x_test.shape[0]
    acc = total_correct/total_number
    test_acc.append(acc)
second_time = time.time()
print(first_time-second_time)

plt.figure()
plt.title('Loss Function Curve')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.plot(train_loss_results, label='$Loss$')
plt.legend()

plt.figure()
plt.title('Acc Curve')
plt.xlabel('Epoch')
plt.ylabel('Acc')
plt.plot(test_acc, label='$Accuracy$')
plt.legend()
plt.show()